{"name":"Italocadamuro.GitHub.io","tagline":"italocadamuro.github.io","body":"## Introduction\r\nThis is my submission for the \"machine Learning\" Course from Coursera.\r\n\r\n* The following chapters have been prepared:\r\n+ Objective\r\n+ Step1 \r\n+ Step2 \r\n+ Conclusions\r\n\r\n\r\n## Objective:\r\nTo fit a model from a training sequence and to apply to model to predict the classification of test sequence. \r\nThe aim is to minimize the classification error.  \r\nIn order to have an estimation of the classification error on the test sequence, I have predict the result the model on the training sequence and evaluated the mis-classification. \r\n\r\n\r\n## Step1 :\r\nThe datasets have been downloaded and a preliminary data analysis performed.\r\nThe data has already been split in two parts, the training sequence is composed of 19622 observations with 160 different predictors.\r\nThe test sequence is just 20 observations with 160 different predictors.\r\nWith the command str I observed that some of 160 predictors showed a very large number of NA. Moreover, a number of predictors showed very little variations. \r\nTherefore, I decided to ignore the predictors with large number of NA and little variance for the first step. \r\nThis decision has been be revisited after a number of predictions have been performed.\r\nIn a further step, I plotted the distribution of some of the predictors and found that they do not have a Gaussian distribution. \r\nFinally, I tried to apply the \"CoxBox\" pre-processing transformation to the non Gaussian predictors but the result was also not Gaussian.\r\n\r\nAs some of the predictors are not close to a Gaussian distribution the methods that require a Gaussian distribution of the predictors are not going to give a good result. \r\n## Step2 :\r\nIn this step I have fit some models using different methods on a subset of the original training data. \r\nFor this step, I have not performed and pre-processor.\r\nResults from predicting results using different methods. \r\nUsing the method \"lda\" without pre-processing, the misclassification for the training sequence was about 5792/19622 = 29%\r\nUsing the method \"rpart\" without pre-processing, the misclassification for the training sequence was about 9898/19622 = 49% ()\r\nFinally, \r\nUsing the method \"rf\" without pre-processing, the misclassification for the training sequence was about 5/19622 = .025% ()\r\n\r\n\r\n\r\n\r\n\r\n## Conclusions:\r\nThe methid \"Random Forest\" offers the better results and it is relatively quick in returning the results. Moreover, I have experienced that the results is not directly dependent from the ntree parameter, as I have seen the same results for ntree=10 and ntree=50.\r\nFinally the method is not affected by the predictors having a Gaussian distribution or not. \r\n\r\nWith \"Random Forest\" the predictions for the test sequence submitted to the Coursera reached 100%. \r\n\r\n\r\n```{r , echo=TRUE}\r\nlibrary(caret)\r\nlibrary(rpart)\r\n## read csv:\r\ntest <- read.csv(file=\"pml-testing.csv\", header=TRUE, sep=\",\")\r\ntraining <- read.csv(file=\"pml-training.csv\", header=TRUE, sep=\",\")\r\n\r\n#\r\n# set.seed\r\n#\r\nset.seed(1812)\r\n\r\n# data inspection:\r\n# summary(training)\r\n# str(training)\r\n```\r\n\r\n\r\n```{r , echo=FALSE}\r\n\r\n# remove measurements that are not relevant (for different reasons):\r\nirrilevant<-c(\"amplitude_yaw_forearm\",\"skewness_yaw_forearm\",\"kurtosis_yaw_forearm\",\"amplitude_yaw_dumbbell\",\"skewness_yaw_dumbbell\",\r\n  \"kurtosis_yaw_dumbbell\",\"amplitude_yaw_belt\",\"skewness_yaw_belt\",\"kurtosis_yaw_belt\")\r\n\r\n# time\r\nirrilevant1<-c(\"raw_timestamp_part_1\", \"raw_timestamp_part_2\", \"cvtd_timestamp\")\r\n\r\n# not enough variation\r\nirrilevant2<- \r\nc(\r\n\"kurtosis_roll_belt\",\r\n\"kurtosis_picth_belt\",\r\n\"skewness_roll_belt\",\r\n\"min_yaw_belt\",\r\n\"kurtosis_roll_arm\", \"kurtosis_picth_arm\", \"kurtosis_yaw_arm\",\r\n\"skewness_roll_arm\", \"skewness_pitch_arm\", \"skewness_yaw_arm\",\r\n\"kurtosis_roll_dumbbell\", \"kurtosis_picth_dumbbell\",\r\n\"kurtosis_yaw_dumbbell\", \"skewness_roll_dumbbell\",\r\n\"skewness_pitch_dumbbell\", \r\n\"min_yaw_dumbbell\",\r\n\"kurtosis_roll_forearm\", \r\n\"skewness_pitch_forearm\",\r\n\"max_yaw_forearm\")\r\n\r\n# (na=19216)\r\nirrilevant3<-c(\r\n\"max_picth_belt\", \"min_roll_belt\", \"min_pitch_belt\", \"amplitude_roll_belt\",\r\n\"amplitude_pitch_belt\", \"var_total_accel_belt\",\"avg_roll_belt\",   \r\n\"stddev_roll_belt\",\"var_roll_belt\",\"avg_pitch_belt\",\"stddev_pitch_belt\",\"var_pitch_belt\",\"avg_yaw_belt\",  \r\n\"stddev_yaw_belt\",\"var_yaw_belt\", \r\n\"var_accel_arm\",\"avg_roll_arm\",     \"stddev_roll_arm\",\"var_roll_arm\",\"avg_pitch_arm\",\"stddev_pitch_arm\", \"var_pitch_arm\",  \r\n\"avg_yaw_arm\",\"stddev_yaw_arm\",\"var_yaw_arm\",   \r\n\"max_roll_arm\", \"max_picth_arm\",\"max_yaw_arm\",\"min_roll_arm\",\"min_pitch_arm\",\"min_yaw_arm\", \r\n\"amplitude_roll_arm\",\"amplitude_pitch_arm\",\"amplitude_yaw_arm\",\r\n\"max_roll_dumbbell\",\r\n\"max_picth_dumbbell\",\"max_yaw_dumbbell\",\"min_roll_dumbbell\",\"min_pitch_dumbbell\",\r\n\"amplitude_roll_dumbbell\",\"amplitude_pitch_dumbbell\",\r\n\"var_accel_dumbbell\",\"avg_roll_dumbbell\",\"stddev_roll_dumbbell\",\"var_roll_dumbbell\",\"avg_pitch_dumbbell\",\r\n\"stddev_pitch_dumbbell\",\"var_pitch_dumbbell\",\"avg_yaw_dumbbell\", \r\n\"stddev_yaw_dumbbell\",\"var_yaw_dumbbell\",\r\n\"max_roll_forearm\",\r\n\"max_picth_forearm\",\r\n\"amplitude_roll_forearm\",\"amplitude_pitch_forearm\",\r\n\"var_accel_forearm\",\"avg_roll_forearm\",\"stddev_roll_forearm\",\r\n\"var_roll_forearm\",\"avg_pitch_forearm\",\"stddev_pitch_forearm\",\r\n\"var_pitch_forearm\",\"avg_yaw_forearm\",\"stddev_yaw_forearm\",\r\n\"var_yaw_forearm\")\r\n\r\n\r\nmoreIrr<- c(\"min_roll_forearm\",\"min_pitch_forearm\",\"min_yaw_forearm\", \r\n            \"kurtosis_picth_forearm\",\"skewness_roll_forearm\",\r\n            \"skewness_roll_belt.1\",\"max_roll_belt\",\"max_yaw_belt\",\r\n            \"X\",\"user_name\",\"new_window\",\"num_window\"   )\r\n\r\n\r\n# very little variation \r\nlittleVar<- c(\r\n\"total_accel_belt\",\r\n\"gyros_belt_x\",\r\n\"gyros_belt_y\", \r\n\"gyros_belt_z\", \r\n\"accel_belt_x\", \r\n\"accel_belt_y\", \r\n\"accel_belt_z\", \r\n\"magnet_belt_x\",\r\n\"magnet_belt_y\",  \r\n\"magnet_belt_z\",  \r\n\"total_accel_arm\",\r\n\"gyros_arm_x\",      \r\n\"gyros_arm_y\",      \r\n\"gyros_arm_z\",      \r\n\"accel_arm_x\",      \r\n\"accel_arm_y\",      \r\n\"accel_arm_z\",          \r\n\"magnet_arm_x\",         \r\n\"magnet_arm_y\",         \r\n\"magnet_arm_z\",         \r\n\"roll_dumbbell\",      \r\n\"total_accel_dumbbell\", \r\n\"gyros_dumbbell_x\",     \r\n\"gyros_dumbbell_y\",     \r\n\"gyros_dumbbell_z\",     \r\n\"accel_dumbbell_x\",     \r\n\"accel_dumbbell_y\",     \r\n\"accel_dumbbell_z\",     \r\n\"magnet_dumbbell_x\",    \r\n\"magnet_dumbbell_y\",    \r\n\"magnet_dumbbell_z\",    \r\n\"total_accel_forearm\",  \r\n\"gyros_forearm_x\",    \r\n\"gyros_forearm_y\",      \r\n\"gyros_forearm_z\",      \r\n\"accel_forearm_x\",      \r\n\"accel_forearm_y\",      \r\n\"accel_forearm_z\",      \r\n\"magnet_forearm_x\",     \r\n\"magnet_forearm_y\",     \r\n\"magnet_forearm_z\" \r\n)\r\n\r\n#allIrr <- c( irrilevant,irrilevant1,irrilevant2,irrilevant3, moreIrr, littleVa)\r\nallIrr <- c( irrilevant,irrilevant1,irrilevant3, moreIrr, littleVar)\r\n\r\n\r\n```\r\n\r\n```{r , echo=TRUE}\r\n\r\nallIrr\r\n\r\n\r\n# show this:\r\ntraining.short <- subset(training, select = setdiff(names(training) ,allIrr))\r\nstr(training.short)\r\n\r\n# check na: \r\nsum(is.na(training.short))\r\n\r\n# check near zero var:\r\nnzv <- nearZeroVar(training.short, saveMetrics=TRUE)\r\nhead(nzv)\r\n\r\n\r\n#\r\n# plot examples, to show that the vars are not gauss.\r\n# \r\n# hist( training.short$accel_dumbbell_y  )\r\n# hist((training.short$total_accel_belt  ))\r\n\r\n\r\n\r\n#\r\n# preprocessing\r\n#\r\npreObj <- preProcess( training.short[,-53], method=c(\"BoxCox\"))\r\n# plot a couple of example:\r\npreObj.short<-predict(preObj, training.short[,-53]) \r\n#hist( preObj.short$accel_dumbbell_y  )          \r\n#hist( preObj.short$total_accel_belt  )\r\n\r\n\r\n```\r\n\r\n```{r, eval=FALSE}\r\n\r\n\r\n\r\ntraining.short1=cbind(training.short [, 53],training.short [, -53])\r\nnn<-names(training.short1)\r\nnn[1]=\"classe\"\r\nnames(training.short1)<-nn\r\n\r\n\r\n# prediction using training.short1\r\nmodFit <- train(training.short1$classe ~ ., method=\"lda\", data=training.short1)\r\npredict(modFit, test)\r\n# 11 predictors [1] E A A A A C C D A E A A B A C B A D D B\r\n# 52 predictos      B A B C C E D D A A D A B A E A A B B B\r\n\r\n# evaluate model against the training\r\ntr<-predict(modFit, training.short1)\r\nsum(tr != training.short1$classe)\r\n#[1] 5792\r\n\r\n\r\nmodFit <- train(training.short1$classe ~ ., method=\"rpart\",data=training.short1)\r\npredict(modFit, test)\r\n# 11 predictors  [1] C A C A A C C A A A C C C A C A A A A C\r\n# 52 predictors      C A C A A C C A A A C C C A C A A A A C\r\n\r\ntr<-predict(modFit, training.short1)\r\nsum(tr != training.short1$classe)\r\n#[1] 9898\r\n\r\nmodFit$finalModel\r\n#n= 19622 \r\n#\r\n#node), split, n, loss, yval, (yprob)\r\n#      * denotes terminal node\r\n#\r\n# 1) root 19622 14042 A (0.28 0.19 0.17 0.16 0.18)  \r\n#   2) roll_belt< 130.5 17977 12411 A (0.31 0.21 0.19 0.18 0.11)  \r\n#     4) pitch_forearm< -33.95 1578    10 A (0.99 0.0063 0 0 0) *\r\n#     5) pitch_forearm>=-33.95 16399 12401 A (0.24 0.23 0.21 0.2 0.12)  \r\n#      10) magnet_dumbbell_y< 439.5 13870  9953 A (0.28 0.18 0.24 0.19 0.11)  \r\n#        20) roll_forearm< 123.5 8643  5131 A (0.41 0.18 0.18 0.17 0.061) *\r\n#        21) roll_forearm>=123.5 5227  3500 C (0.077 0.18 0.33 0.23 0.18) *\r\n#      11) magnet_dumbbell_y>=439.5 2529  1243 B (0.032 0.51 0.043 0.22 0.19) *\r\n#   3) roll_belt>=130.5 1645    14 E (0.0085 0 0 0 0.99) *\r\n#\r\n\r\n\r\n# predict using Random Forest:\r\nmodFit <- train(training.short1$classe ~ ., method=\"rf\",ntree=10, data=training.short1)\r\npredict(modFit, test)\r\n# ntree =  10  B A B A A E D B A A B C B A E E A B B B\r\n# ntree =  50  B A B A A E D B A A B C B A E E A B B B\r\n\r\n\r\n# evaluate model against the training\r\ntr<-predict(modFit, training.short1)\r\nsum(tr != training.short1$classe)\r\n# [1] 5\r\nt<-(tr != training.short1$classe)\r\n#training.short1[t,]\r\n#      classe roll_belt pitch_belt yaw_belt total_accel_belt gyros_belt_x gyros_belt_y\r\n#3975       A      0.50       4.75    -88.2                4         0.03         0.02\r\n#7079       B      1.03       5.23    -87.8                4        -0.10         0.00\r\n#8619       B    122.00     -44.50    173.0               17         0.03         0.13\r\n#8620       B    122.00     -44.50    173.0               17         0.03         0.11\r\n#15286      D    126.00     -41.40    164.0               18         0.11         0.11\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}